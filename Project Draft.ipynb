{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SEC website has index files that provide information on all filings avaliable through EDGAR.\n",
    "\n",
    "We will combine these index files to make a database of filings. This database contains information about these filings and the URLs. Building this database allows us to create samples of filings that we wish to download. Alternatively, you can use the WRDS module. \n",
    "\n",
    "After we have imported our modules, we need to generate the URLs needed to scrape the index files. We know the first part of the URL will stay static. It will be https://www.sec.gov/Archives/edgar/full-index/; however, the second part of the URL will change. Thankfully the second part follows an API [  /X/QTR(Y)/master.idx  ] with X being the year, and Y being the QTR number. Since our URLs follow a predefined pattern, we can create a for loop to generate URLs for as many years as we would like. For this walkthrough, however, I have limited the database to 2019. We can expand the range by changing the variables.\n",
    "\n",
    "Once we have generated our dictionary of URLs with the for loop, we create a list to store the files already downloaded in\n",
    "the index folder and a dictionary to store the information about the data we need to download. This list will prevent you from downloading these files every time you run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import spacy\n",
    "import unidecode, requests, unidecode, tqdm\n",
    "import lxml.html\n",
    "from glob import glob\n",
    "import os, re, sys, time\n",
    "\n",
    "#Url_dict will store filename, URL\n",
    "url_dict = {}\n",
    "url_base = 'https://www.sec.gov/Archives/edgar/full-index/'\n",
    "\n",
    "#Set our data path to the filings folder\n",
    "data_path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#Variables to configure the for loop, loop is configured to only download 2019 files currently.\n",
    "start_year = 2019\n",
    "end_year = 2020\n",
    "date_range = end_year - start_year\n",
    "\n",
    "#loops through each year, then for each year loops through 4 times.\n",
    "for i in range(date_range):\n",
    "    for i in range(1, 5):\n",
    "        dict_key = str(start_year) + '_Q' + str(i)+ '_Master.idx'\n",
    "        url = url_base + str(start_year) + '/QTR' + str(i) +'/master.idx'\n",
    "        url_dict[dict_key] = url\n",
    "        \n",
    "#os.listdir will create a list of files present in the folder of the path it is passed.\n",
    "files_down = os.listdir(data_path)        \n",
    "#Store downloaded files\n",
    "download_dict = {}\n",
    "\n",
    "#Loops through Url_dict and checks if files are in the Index File Foler, if they are not it downloads them and adds them.\n",
    "#.items() is required to loop through dictionaries. We are also required to specificy two variable names before naming\n",
    "#the data structure we wish to alter. For simplicity I usually utilize key, and value. \n",
    "for key, value in url_dict.items():\n",
    "    if key not in files_down:\n",
    "        download_dict[key] = value\n",
    "\n",
    "#Get out data_path\n",
    "data_path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#data_dict will store the raw text data\n",
    "data_dict = {}\n",
    "\n",
    "#Loop through out download dict and download the items contained in it, if we can not get one it will return an error.\n",
    "for key, value in download_dict.items():\n",
    "    #requests.get generates an object. Objects store a variety of information that can be acessed through commands.\n",
    "    res = requests.get(value)\n",
    "    \n",
    "    #Two examples of this are .status code and .txt\n",
    "    if res.status_code == 200:\n",
    "        print('Found...... Downloading  ' + str(key))\n",
    "        res = res.text\n",
    "        data_dict[key] = res\n",
    "        \n",
    "    else:\n",
    "        print('Error.....' + str(key))\n",
    "\n",
    "#Loop through the data_dict and remove everything before CIK, and the ------- in the document. Save to Index File folder\n",
    "#This loop is just for formatting, dont worry about it too much. \n",
    "for key, text in data_dict.items():\n",
    "    text_2 = text.split('CIK')\n",
    "    text = text_2[1]\n",
    "    text = 'CIK' + text\n",
    "    text = re.sub(\"-\",\"\", text, count=80)\n",
    "    filename = key\n",
    "    html_file = open(join(data_path, filename), 'a')\n",
    "    html_file.write(text)\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to take the index files we downloaded and altered, and put them into a dataframe.\n",
    "\n",
    "We will begin by creating another list containing the files downloaded in the Index Files folder. This list is files_0, and we will use this list to gather the names of the files. We will also create a second list files_1; we will use it to store the actual paths of the files in files_0. We will build these paths by utilizing a for loop, where we can combine the folder's path with a backslash and the file name to create the path of the file. Interestingly we cannot add a single backslash to the path and file because a single backslash is a special character in Python. Instead, we have to pass two backslashes, which Python will recognize as one. \n",
    "\n",
    "Once we have the files_1 list, we can create a dataframe that stores all of the information from these files. We will do some minor housekeeping with a command to add back leading zeros to the CIK number, reformatting the date from a string to a DateTime format in case we want to create a sample based on date. We also need to create the URLs for the filings. The index files only come with half of the information needed to generate the URL. Luckily we can add the first half of the URL, which is not unique to the second half of the URL contained within the index filing, which is unique. We also do some housekeeping with Company names, removing special characters so we can use the company name to create file names later. Pandas will automatically truncate values to save memory, so we will need to stop it from truncating these values.\n",
    "\n",
    "We will finally save the dataframe to the folder in which the program is running. This will save us from having to repeat this process in the future. We save this a .pkl file, which is short for the Pickle file extension. This file extension serializes data in Python and stores it in a way that it can be loaded into another Python script without having to reformat the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Form Type</th>\n",
       "      <th>Date Filed</th>\n",
       "      <th>Filename</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>edgar/data/1000045/0001193125-19-039489.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1000045/0001193125-19-039489.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>edgar/data/1000045/0001357521-19-000001.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>edgar/data/1000045/0001357521-19-000002.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>edgar/data/1000045/0001357521-19-000003.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>edgar/data/1000045/0001193125-19-024617.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1000045/0001193125-19-024617.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206005</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>edgar/data/9984/0000009984-19-000109.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000109.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206006</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>edgar/data/9984/0000009984-19-000113.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000113.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206007</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>edgar/data/9984/0000009984-19-000114.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000114.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206008</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>edgar/data/9984/0000009984-19-000098.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000098.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206009</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>edgar/data/9984/0000898822-19-000104.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9984/0000898822-19-000104.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741946 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CIK            Company Name Form Type Date Filed  \\\n",
       "0       0001000045  NICHOLAS FINANCIAL INC  10-Q     2019-02-14   \n",
       "1       0001000045  NICHOLAS FINANCIAL INC  4        2019-01-15   \n",
       "2       0001000045  NICHOLAS FINANCIAL INC  4        2019-02-19   \n",
       "3       0001000045  NICHOLAS FINANCIAL INC  4        2019-03-15   \n",
       "4       0001000045  NICHOLAS FINANCIAL INC  8-K      2019-02-01   \n",
       "...            ...                     ...  ...             ...   \n",
       "206005  0000009984  BARNES GROUP INC        4        2019-12-02   \n",
       "206006  0000009984  BARNES GROUP INC        4        2019-12-04   \n",
       "206007  0000009984  BARNES GROUP INC        4        2019-12-12   \n",
       "206008  0000009984  BARNES GROUP INC        8-K      2019-10-25   \n",
       "206009  0000009984  BARNES GROUP INC        8-K      2019-12-20   \n",
       "\n",
       "                                           Filename  \\\n",
       "0       edgar/data/1000045/0001193125-19-039489.txt   \n",
       "1       edgar/data/1000045/0001357521-19-000001.txt   \n",
       "2       edgar/data/1000045/0001357521-19-000002.txt   \n",
       "3       edgar/data/1000045/0001357521-19-000003.txt   \n",
       "4       edgar/data/1000045/0001193125-19-024617.txt   \n",
       "...                                             ...   \n",
       "206005  edgar/data/9984/0000009984-19-000109.txt      \n",
       "206006  edgar/data/9984/0000009984-19-000113.txt      \n",
       "206007  edgar/data/9984/0000009984-19-000114.txt      \n",
       "206008  edgar/data/9984/0000009984-19-000098.txt      \n",
       "206009  edgar/data/9984/0000898822-19-000104.txt      \n",
       "\n",
       "                                                                             URL  \n",
       "0       https://www.sec.gov/Archives/edgar/data/1000045/0001193125-19-039489.txt  \n",
       "1       https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000001.txt  \n",
       "2       https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000002.txt  \n",
       "3       https://www.sec.gov/Archives/edgar/data/1000045/0001357521-19-000003.txt  \n",
       "4       https://www.sec.gov/Archives/edgar/data/1000045/0001193125-19-024617.txt  \n",
       "...                                                                          ...  \n",
       "206005  https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000109.txt     \n",
       "206006  https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000113.txt     \n",
       "206007  https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000114.txt     \n",
       "206008  https://www.sec.gov/Archives/edgar/data/9984/0000009984-19-000098.txt     \n",
       "206009  https://www.sec.gov/Archives/edgar/data/9984/0000898822-19-000104.txt     \n",
       "\n",
       "[741946 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set our data path to the filings folder\n",
    "path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#Create List of Files in Index Files\n",
    "files_0 = os.listdir(path)\n",
    "\n",
    "#Second List to add path to Index Files\n",
    "files_1 = []\n",
    "\n",
    "#Add Path to Index \n",
    "for file in files_0:\n",
    "    file = path + '\\\\' + file\n",
    "    files_1.append(file)\n",
    "files_1\n",
    "\n",
    "#Create Data Frame\n",
    "Edgar_df = pd.concat((pd.read_table(file, encoding=\"latin1\", sep='|') for file in files_1))\n",
    "\n",
    "#Add back leading zeros\n",
    "Edgar_df['CIK'] = Edgar_df['CIK'].apply(lambda x: '{0:0>10}'.format(x))\n",
    "\n",
    "#Need to Format Date Filed as a datetime, so we can search it later. \n",
    "Edgar_df['Date Filed'] =  pd.to_datetime(Edgar_df['Date Filed'])\n",
    "\n",
    "#Trying adding URL to df, may be able to feed directly into a downloader\n",
    "Edgar_df['URL'] = 'https://www.sec.gov/Archives/' + Edgar_df['Filename']\n",
    "\n",
    "#Remove special characters from the company names, some of these can cause problems\n",
    "Edgar_df['Company Name'].replace('[^A-Za-z0-9- ]+', '', regex=True, inplace=True)\n",
    "\n",
    "#Stops Pandas from truncating values.\n",
    "pd.set_option('display.max_colwidth', -1) # Was using -1, but being depreciated\n",
    "\n",
    "#Save the dataframe so we can open it again later without having to recreate it. \n",
    "Edgar_df.to_pickle('Edgar_df.pkl')\n",
    "\n",
    "Edgar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this dataframe to create samples, but for this tutorial, we will use a preselected sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Form Type</th>\n",
       "      <th>Date Filed</th>\n",
       "      <th>Filename</th>\n",
       "      <th>URL</th>\n",
       "      <th>Save_Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>File ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001522420</td>\n",
       "      <td>BSB Bancorp Inc</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>edgar/data/1522420/0001193125-19-076573.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1522420/0001193125-19-076573.txt</td>\n",
       "      <td>BSB Bancorp Inc 10-K 2019-03-15.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000896264</td>\n",
       "      <td>USANA HEALTH SCIENCES INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>edgar/data/896264/0001047469-19-000707.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/896264/0001047469-19-000707.txt</td>\n",
       "      <td>USANA HEALTH SCIENCES INC 10-K 2019-02-26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000719220</td>\n",
       "      <td>ST BANCORP INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>edgar/data/719220/0000719220-19-000017.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/719220/0000719220-19-000017.txt</td>\n",
       "      <td>ST BANCORP INC 10-K 2019-02-21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001280784</td>\n",
       "      <td>Hercules Capital Inc</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>edgar/data/1280784/0001564590-19-003680.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1280784/0001564590-19-003680.txt</td>\n",
       "      <td>Hercules Capital Inc 10-K 2019-02-21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000311817</td>\n",
       "      <td>HMG COURTLAND PROPERTIES INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>edgar/data/311817/0001575872-19-000071.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/311817/0001575872-19-000071.txt</td>\n",
       "      <td>HMG COURTLAND PROPERTIES INC 10-K 2019-03-28.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0001178670</td>\n",
       "      <td>ALNYLAM PHARMACEUTICALS INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>edgar/data/1178670/0001564590-19-003022.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1178670/0001564590-19-003022.txt</td>\n",
       "      <td>ALNYLAM PHARMACEUTICALS INC 10-K 2019-02-14.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0001590364</td>\n",
       "      <td>Fortress Transportation  Infrastructure Investors LLC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>edgar/data/1590364/0001590364-19-000002.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1590364/0001590364-19-000002.txt</td>\n",
       "      <td>Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0001428439</td>\n",
       "      <td>ROKU INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>edgar/data/1428439/0001564590-19-005829.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1428439/0001564590-19-005829.txt</td>\n",
       "      <td>ROKU INC 10-K 2019-03-01.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0001293282</td>\n",
       "      <td>TechTarget Inc</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>edgar/data/1293282/0001564590-19-007403.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1293282/0001564590-19-007403.txt</td>\n",
       "      <td>TechTarget Inc 10-K 2019-03-13.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0001451512</td>\n",
       "      <td>Terra Tech Corp</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>edgar/data/1451512/0001477932-19-000995.txt</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/1451512/0001477932-19-000995.txt</td>\n",
       "      <td>Terra Tech Corp 10-K 2019-03-15.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                CIK                                           Company Name  \\\n",
       "File ID                                                                      \n",
       "0        0001522420  BSB Bancorp Inc                                         \n",
       "1        0000896264  USANA HEALTH SCIENCES INC                               \n",
       "2        0000719220  ST BANCORP INC                                          \n",
       "3        0001280784  Hercules Capital Inc                                    \n",
       "4        0000311817  HMG COURTLAND PROPERTIES INC                            \n",
       "...             ...                   ...                                    \n",
       "95       0001178670  ALNYLAM PHARMACEUTICALS INC                             \n",
       "96       0001590364  Fortress Transportation  Infrastructure Investors LLC   \n",
       "97       0001428439  ROKU INC                                                \n",
       "98       0001293282  TechTarget Inc                                          \n",
       "99       0001451512  Terra Tech Corp                                         \n",
       "\n",
       "        Form Type  Date Filed                                     Filename  \\\n",
       "File ID                                                                      \n",
       "0        10-K      2019-03-15  edgar/data/1522420/0001193125-19-076573.txt   \n",
       "1        10-K      2019-02-26  edgar/data/896264/0001047469-19-000707.txt    \n",
       "2        10-K      2019-02-21  edgar/data/719220/0000719220-19-000017.txt    \n",
       "3        10-K      2019-02-21  edgar/data/1280784/0001564590-19-003680.txt   \n",
       "4        10-K      2019-03-28  edgar/data/311817/0001575872-19-000071.txt    \n",
       "...       ...             ...                                          ...   \n",
       "95       10-K      2019-02-14  edgar/data/1178670/0001564590-19-003022.txt   \n",
       "96       10-K      2019-02-28  edgar/data/1590364/0001590364-19-000002.txt   \n",
       "97       10-K      2019-03-01  edgar/data/1428439/0001564590-19-005829.txt   \n",
       "98       10-K      2019-03-13  edgar/data/1293282/0001564590-19-007403.txt   \n",
       "99       10-K      2019-03-15  edgar/data/1451512/0001477932-19-000995.txt   \n",
       "\n",
       "                                                                              URL  \\\n",
       "File ID                                                                             \n",
       "0        https://www.sec.gov/Archives/edgar/data/1522420/0001193125-19-076573.txt   \n",
       "1        https://www.sec.gov/Archives/edgar/data/896264/0001047469-19-000707.txt    \n",
       "2        https://www.sec.gov/Archives/edgar/data/719220/0000719220-19-000017.txt    \n",
       "3        https://www.sec.gov/Archives/edgar/data/1280784/0001564590-19-003680.txt   \n",
       "4        https://www.sec.gov/Archives/edgar/data/311817/0001575872-19-000071.txt    \n",
       "...                                                                           ...   \n",
       "95       https://www.sec.gov/Archives/edgar/data/1178670/0001564590-19-003022.txt   \n",
       "96       https://www.sec.gov/Archives/edgar/data/1590364/0001590364-19-000002.txt   \n",
       "97       https://www.sec.gov/Archives/edgar/data/1428439/0001564590-19-005829.txt   \n",
       "98       https://www.sec.gov/Archives/edgar/data/1293282/0001564590-19-007403.txt   \n",
       "99       https://www.sec.gov/Archives/edgar/data/1451512/0001477932-19-000995.txt   \n",
       "\n",
       "                                                                         Save_Name  \n",
       "File ID                                                                             \n",
       "0        BSB Bancorp Inc 10-K 2019-03-15.txt                                        \n",
       "1        USANA HEALTH SCIENCES INC 10-K 2019-02-26.txt                              \n",
       "2        ST BANCORP INC 10-K 2019-02-21.txt                                         \n",
       "3        Hercules Capital Inc 10-K 2019-02-21.txt                                   \n",
       "4        HMG COURTLAND PROPERTIES INC 10-K 2019-03-28.txt                           \n",
       "...                                           ...                                   \n",
       "95       ALNYLAM PHARMACEUTICALS INC 10-K 2019-02-14.txt                            \n",
       "96       Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt  \n",
       "97       ROKU INC 10-K 2019-03-01.txt                                               \n",
       "98       TechTarget Inc 10-K 2019-03-13.txt                                         \n",
       "99       Terra Tech Corp 10-K 2019-03-15.txt                                        \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load predefined sample\n",
    "Sample_df = pd.read_pickle('Sample_df.pkl')\n",
    "\n",
    "# Update Index for Identifer, with an Identifer the df will play nicely with flow control\n",
    "Sample_df = Sample_df.reset_index(drop=True)\n",
    "Sample_df.index.name = 'File ID'\n",
    "\n",
    "#Convert Date Filed back to a string so we can combine it with Company Name, and Form Type\n",
    "#to get our save name.\n",
    "Sample_df['Date Filed'] = Sample_df['Date Filed'].astype(str)\n",
    "Sample_df['Save_Name'] = Sample_df['Company Name'] + ' ' + Sample_df['Form Type'] + ' ' + Sample_df['Date Filed'] + '.txt'\n",
    "Sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to download the 10-K filings from the SEC website.\n",
    "\n",
    "We start by defining a function, download_file. This function will take the URL, date, form, and company name, and return the text and status code. This function is originally from Dr. Kok's GitHub and has been modified for our purposes.\n",
    "\n",
    "After we have defined our download function, we will get ready to download the files. We will begin by setting our path equal to the path of the folder where we will store our filings. This will allow us to check for any filings present in the folder, and tell our program where to store them once we have downloaded them.\n",
    "\n",
    "We also need to create two dictionaries to store the information for use in our program. I utilize dictionaries a lot in my programs because information can be stored by an identifier called a key. For example, you may save a cleaned 10-k in a dictionary using the company name as the key, which gives you a unique identifier to call the information as opposed to a list where you can only call stored information by the index where it is stored. Dictionaries and lists can be combined however, to store multiple pieces of information under one key. I have found it is often better to use multiple dictionaries than to combine data structures as combining data structures creates some unique problems and messy code. Sometimes it is necessary and more convenient, but it is rare.\n",
    "\n",
    "Once we have initialized our data structures and have our path and files list, we can download our files. We will create a for loop for this task. Once again, we will need to pass two variables before we declare the data structure we wish to alter. We will also have to do something new with this for loop, we will need to call the function .iterrows() for our data structure, this is because we are dealing with a dataframe object and not a dictionary, the .itterows() function, functions similarly to the .items() function. The for loop contains two if statements, we want to check if the file is saved in our index folder, and if it is we will open the file, and if it is not, we will download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to downlaod files\n",
    "def download_file(url, date, form, company, max_tries=4, sleep_time = 1):\n",
    "    failed_attempts = 0\n",
    "    while True: \n",
    "        res = requests.get(url)\n",
    "        \n",
    "        #download the raw html of the file we just scraped\n",
    "        if '.txt' in url:\n",
    "            # Define filename\n",
    "            filename = company + ' ' + form + ' ' + date + '.txt'\n",
    "            \n",
    "            #Create file\n",
    "            html_file = open(join(data_path, filename), 'a')\n",
    "            \n",
    "            #Decode res object\n",
    "            resx = unidecode.unidecode(res.text)\n",
    "            \n",
    "            #Save file\n",
    "            html_file.write(str(resx))\n",
    "            \n",
    "            #Close file\n",
    "            html_file.close()\n",
    "            \n",
    "        #Status_code = 200 means sucessful scraping\n",
    "        if res.status_code == 200:  \n",
    "            return True, res.text\n",
    "        \n",
    "        #Loop will attempt to download 3 more times, if failure has been encountered \n",
    "        else:\n",
    "            if failed_attempts < max_tries:\n",
    "                failed_attempts += 1\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                return False, 'Could not download'\n",
    "\n",
    "\n",
    "#Set our data path to the filings folder\n",
    "data_path = join(os.getcwd(), 'Filings')\n",
    "\n",
    "#List of filenames in the filing folder\n",
    "files_down = os.listdir(data_path)\n",
    "\n",
    "result_10k_dict = {}\n",
    "status_dict = {}\n",
    "\n",
    "for index, row in Sample_df.iterrows():\n",
    "        \n",
    "    #Load file list incase program is used out of order    \n",
    "    files_down = os.listdir(data_path)\n",
    "    \n",
    "    #if program exists, load it instead of downloading it again\n",
    "    if row['Save_Name'] in files_down:\n",
    "        with open(join(data_path, row['Save_Name']), 'r') as file:\n",
    "            file_content = file.read()\n",
    "            result_10k_dict[row['Save_Name']] = file_content\n",
    "            \n",
    "    #if program does not exist then call the download function    \n",
    "    if row['Save_Name'] not in files_down:\n",
    "        download_res = download_file(row['URL'], row['Date Filed'], row['Form Type'], row['Company Name'])\n",
    "    \n",
    "        status_dict[index] = download_res[0]\n",
    "    \n",
    "        if download_res[0]:\n",
    "            result_10k_dict[row['Save_Name']] = download_res[1]\n",
    "                   \n",
    "#The if not in x part of the loop works correctly and the files download, need to test after they have been saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****This section of code is largely taken from Dr. Kok's github. It is the pipeline to process the files we have downloaded into clean text. I haven't spent a large amount of time on this section of code, and therefore my understanding of it is currently subpar.\n",
    "\n",
    "We start be defining a pattern_dict, this dictionary contains regular expressions statements. \n",
    "\n",
    "Regular expressions are a very valuable, and very frustrating tool. They are confusing and unintuitve, and they will only make sense after practice, like organic chemistry. I would recommend reading the Python documentation for regular expressions here: https://docs.python.org/3/library/re.html.\n",
    "\n",
    "We will use this dictionary to extract metadata from 10-k documents. We do this through the extract_metadata function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17045\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "#Define regular expressions dictionary\n",
    "pattern_dict = {\n",
    "    'documents' : re.compile(r\"<document>(.*?)</document>\", re.IGNORECASE | re.DOTALL),\n",
    "    'metadata' : {\n",
    "        'type' : re.compile(r\"<type>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'sequence' : re.compile(r\"<sequence>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'Filename' : re.compile(r\"<filename>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'description' : re.compile(r\"<description>(.*?)\\n\", re.IGNORECASE | re.DOTALL)\n",
    "    },\n",
    "    'text' : re.compile(r\"<text>(.*?)</text>\", re.IGNORECASE | re.DOTALL)\n",
    "}\n",
    "\n",
    "#Define extract_metadata function\n",
    "def extract_metadata(doc, pattern_dict=pattern_dict):\n",
    "    data_dict = {}\n",
    "    \n",
    "    data_dict['metadata'] = {}\n",
    "    for key, pattern in pattern_dict['metadata'].items():\n",
    "        matches = pattern.findall(doc)\n",
    "        if matches:\n",
    "            data_dict['metadata'][key] = matches[0]\n",
    "        else:\n",
    "            data_dict['metadata'][key] = np.nan\n",
    "            \n",
    "    text_match = pattern_dict['text'].findall(doc)\n",
    "    if text_match:\n",
    "        data_dict['text'] = text_match[0]\n",
    "    else:\n",
    "        data_dict['text'] = np.nan\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "data_10k_dict = {}\n",
    "for Filename, data in result_10k_dict.items():\n",
    "    docs_split = pattern_dict['documents'].findall(data)\n",
    "        \n",
    "    for doc in docs_split:\n",
    "        doc_data = extract_metadata(doc)\n",
    "        \n",
    "        ## Only keep 10-K document\n",
    "        if doc_data['metadata']['type'] == '10-K':\n",
    "            data_10k_dict[Filename] = doc_data['text']\n",
    "            break\n",
    "            \n",
    "html_10k_dict = {}\n",
    "text_10k_dict = {}\n",
    "for Filename, raw_text in data_10k_dict.items():\n",
    "    html = lxml.html.fromstring(raw_text)\n",
    "    html_10k_dict[Filename] = html\n",
    "    text_10k_dict[Filename] = html.text_content()\n",
    "    \n",
    "path = join(os.getcwd(), 'Clean Files')\n",
    "cleantext_10k_dict = {}\n",
    "for Filename, text in text_10k_dict.items():\n",
    "    ## Fix encoding\n",
    "    clean_text = unidecode.unidecode(text)\n",
    "    \n",
    "    ## Replace newline characters with space\n",
    "    clean_text = re.sub('\\s', ' ', clean_text)\n",
    "    \n",
    "    ## Remove duplicate whitespaces\n",
    "    clean_text = ' '.join([word for word in clean_text.split(' ') if word])\n",
    "    \n",
    "    ## Replace \"Page number + Table of Contents footer\"\n",
    "    clean_text = re.sub(' \\d+ Table of Contents ', ' ', clean_text)\n",
    "    \n",
    "    cleantext_10k_dict[Filename] = clean_text\n",
    "    \n",
    "    html_file = open(join(path, Filename), 'w')\n",
    "    resx = unidecode.unidecode(text)\n",
    "\n",
    "    html_file.write(str(resx))\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to shorten the clean text files we just produced and store them in a new dictionary, appropriately named cleantext_10k_dict_short. By doing this, we can significantly decrease the amount of time our program takes to run. Since the AMV is listed early in the 10-K filing, we can safely remove 4/5s of the 10-K filing without risking data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext_10k_dict_short = {}\n",
    "\n",
    "#Loop through cleantext_10k_dict and take the first 1/5 of the document and store it in cleantext_10k_dict_short\n",
    "for key, text in cleantext_10k_dict.items():\n",
    "    doc = str(cleantext_10k_dict[key])\n",
    "    length = len(doc)\n",
    "    new_len = length * .2\n",
    "    new_len = int(float(new_len))\n",
    "    doc = doc[0:new_len]\n",
    "    cleantext_10k_dict_short[key] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the clean text and search for the text's aggregate market value using a for loop and a regular expression. We start the loop by creating a variable to hold our text, doc, and set doc equal to the clean text passed through a str() function. This is because the clean text is stored as a request object. Once we have the doc as a string, we can also pass the .lower() function, which changes any capital letters to lower case letters. This way, we do not need to worry about case sensitivity. We then define our regular expressions in the result variable; we give the regular expressions two cases to search for separated by the '|' character. The first expression tells the program to search for aggregate market value, a number, a decimal, two more numbers, then any 7 characters. This is meant to look for numbers such as 7.0 billion or 8.00 million. The second regular expression looks for a sentence containing 'aggregate market value' and returns that sentence. We then create an if statement to eliminate any text documents that did not return a result. If a result was not found, the function will return '[]'; therefore, we can tell the if statement if the result does not equal '[]' then we want to store that result in our amv dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' yes no the aggregate market value of the voting and non-voting common equity held by nonaffiliates as of june 30, 2018 was approximately $276,496,828.']\n",
      "[\" yes no the aggregate market value of the voting and non-voting common stock held by non-affiliates of the registrant as of the last business day of the registrant's most recently completed second fiscal quarter was approximately $1.2 billion\"]\n",
      "[\" yes o no xstate the aggregate market value of the voting and non-voting common equity held by non-affiliates computed by reference to the price at which the common equity was last sold, or the average bid and asked price of such common equity, as of the last business day of the registrant's most recently completed second fiscal quarter.\"]\n"
     ]
    }
   ],
   "source": [
    "amv_dict = {}\n",
    "#Loops through the short dict and searchs for the aggregate market value in sentences\n",
    "for key, item in cleantext_10k_dict_short.items():\n",
    "    #Get String of the doc\n",
    "    doc = str(cleantext_10k_dict_short[key])\n",
    "    \n",
    "    #Make the doc lowercase so we don't have to worry about case sensitivity\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    #Search for aggergate market value followed by X.X XXXXXXXX then just aggregate market value in a sentence\n",
    "    #First case will return the million/billion values if present\n",
    "    result = re.findall(r\"([^.]*?aggregate market value[^.]*\\.[0-9]........|[^.]*?aggregate market value[^.]*\\.)\", doc)\n",
    "    \n",
    "    #Only save if we have a result\n",
    "    if str(result) != '[]':\n",
    "        amv_dict[key] = result\n",
    "\n",
    "#Dictionary examples\n",
    "print(amv_dict['BSB Bancorp Inc 10-K 2019-03-15.txt'])\n",
    "print(amv_dict['Hercules Capital Inc 10-K 2019-02-21.txt'])\n",
    "print(amv_dict['ST BANCORP INC 10-K 2019-02-21.txt'])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now stored the sentence that likely contains the aggregate market value information in the amv_dictionary. We now need to start extracting the numbers from that sentence. This is quite easy for the sentences that say $7.0 billion or $8 million, but quite difficult for the ones that say: Our AMV was approximately $1,302,302 based on a total share number of 304,303, trading at X price. We will cover the millions and billions right now and pick up this conversation in the next code segment.\n",
    "\n",
    "We are going to create 4 dictionaries to store information. The functions and data collection in the following section can and will be cleaned up significantly. We will use match_dict to store all of the numbers we extract from these sentences, and we will use match_dict_mb to store strings such as '$4 million' and $8 billion'. million_dict will be used to store the strings in denominations of millions, and billions_dict will be used to store the strings in the billions.\n",
    "\n",
    "We will use a for loop and regular expression to extract the strings in the form of '$4 million' and '$8 billion' from the amv_dictionary. We will then store these in the match_dict_mb and use another for loop and a couple of if statements to sort these into the million_dict and the billion_dict. We will sort these simply by checking if 'mill' or 'bill' is in the text itself. We use a shorter string than 'million' or 'billion' because these filings often contain typos.\n",
    "\n",
    "We finish by using a for loop to extract any number present in a sentence from the amv_dict and store these numbers in the match_dict. We will use this match_dict to attempt to extract the AMV from the rest of the companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30,', '2018', '276,496,828.']\n",
      "['$59.0 million']\n",
      "['$1.2 billion']\n"
     ]
    }
   ],
   "source": [
    "match_dict = {}\n",
    "match_dict_mb = {}\n",
    "million_dict = {}\n",
    "billion_dict = {}\n",
    "\n",
    "#Search the amv_dict for the millions and billions value and store them in match_dict_mb\n",
    "for key, text in amv_dict.items():\n",
    "    doc = str(amv_dict[key])\n",
    "    match = re.findall(r\"(\\$[.\\d,]+........)\", doc)\n",
    "    if match:\n",
    "        match_dict_mb[key] = str(match)\n",
    "\n",
    "#Loop through match_dict_mb if mill in the sentence store in million dict, if bill store in billions dict\n",
    "for key, text in match_dict_mb.items():\n",
    "    doc = str(match_dict_mb[key])\n",
    "    if 'mill' in text:\n",
    "        million_dict[key] = text\n",
    "    if 'bill' in text:\n",
    "        billion_dict[key] = text\n",
    "        \n",
    "#Extract just the numerical values from all the sentences in amv_dict\n",
    "for key, text in amv_dict.items():\n",
    "    doc = str(amv_dict[key])\n",
    "    match = re.findall(r\"([.\\d,]+)\", doc)\n",
    "    if match:\n",
    "        match_dict[key] = match\n",
    "\n",
    "#Examples\n",
    "print(match_dict['BSB Bancorp Inc 10-K 2019-03-15.txt'])\n",
    "print(million_dict['SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt'])\n",
    "print(billion_dict['Hercules Capital Inc 10-K 2019-02-21.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dictionary containing the numbers extracted from the sentences in the amv dict. We need to convert the list into a string and remove special characters so we can convert it into an integer. The results that are stored in match_dict are lists. We want to loop through this list and do the necessary formatting while keeping the data structure. To accomplish this we will use a nested for loop, the first for loop will loop through the dictionary, and the second will loop through the list stored under the dictionary key. Once we have accomplished this, we will utilize another nested for loop to convert the strings into integers. \n",
    "\n",
    "At this point, our lists can have up to 4 numbers in them: Year, AMV, Share Price, and Number of Shares. My current approach is to delete any numbers less than 10,000. This will get rid of the Year, and Share Price. It should also eliminate most entries in the million or billion dictionaries. In the case of one of these filings listing share number above 10,000 the results can be cross-referenced to reduce error. \n",
    "\n",
    "If your paying attention, you have probably noticed an issue in the algorithm. The number of shares can be higher than the AMV. We will get two errors out of 68 observations because of this. I am working on a couple of solutions around this flaw. I am sure a few more imperfections will be discovered as the algorithm is expanded to a larger sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276496828\n",
      "276496828\n"
     ]
    }
   ],
   "source": [
    "#Loop through match_dict and remove special characters\n",
    "#Can clean and shorten this\n",
    "for key, text in match_dict.items():\n",
    "    item_lis = []\n",
    "    for item in text:\n",
    "        doc = item\n",
    "        doc = str(doc)\n",
    "        doc = doc.replace('[', '')\n",
    "        doc = doc.replace('[', '')\n",
    "        doc = doc.replace(']', '')\n",
    "        doc = doc.replace('\"', '')\n",
    "        doc = doc.replace(\"'\", '')\n",
    "        doc = doc.replace(\" \", '|')\n",
    "        doc = doc.replace(\",\", '')\n",
    "        doc = doc.replace(r\".\", '')\n",
    "        doc = str(doc)\n",
    "        doc = str(doc)\n",
    "        item_lis.append(doc)\n",
    "        match_dict[key] = item_lis\n",
    "\n",
    "#Loop through match_dict delete blank values, and convert integers to spaces\n",
    "for key,text in match_dict.items():\n",
    "    doc = match_dict[key]\n",
    "    num_lis = []\n",
    "    for item in doc:\n",
    "        if item == ' ' or item == '':\n",
    "            item = int(0)\n",
    "            num_lis.append(item)\n",
    "        else:\n",
    "            item = int(item)\n",
    "            num_lis.append(item)\n",
    "    num_lis = list(map(int, num_lis))\n",
    "    num_lis.sort(reverse=True)\n",
    "    num = num_lis[0]\n",
    "    match_dict[key] = num\n",
    "\n",
    "likely_dict = {}\n",
    "    \n",
    "for key, text in match_dict.items():\n",
    "    item = match_dict[key]\n",
    "    if item > 10000:\n",
    "        likely_dict[key] = item\n",
    "\n",
    "#Examples\n",
    "print(match_dict['BSB Bancorp Inc 10-K 2019-03-15.txt'])\n",
    "print(likely_dict['BSB Bancorp Inc 10-K 2019-03-15.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process the filings that contain $X.X million in the AMV sentence. Processing these filings is significantly easier than the likely, as we are almost certain the number we will extract will be the actual AMV.\n",
    "\n",
    "We begin by extracting decimal numbers from the million dict and store the numbers back in the dict. While I say they are numbers, they are, in fact, lists in Python, as we have used a regular expression to return the results. \n",
    "\n",
    "We should have only returned one decimal number from this match. To make sure we create another dictionary million_dict_step2 and only store the results that have returned a single number in the dictionary.\n",
    "\n",
    "We then need to convert the list to a string and format it as we did above. In this case, the only difference is that we are not concerned about keeping the list data structure intact because we are only dealing with one item. \n",
    "\n",
    "Finally, we convert the string into an integer and multiply it by a million to return the final AMV value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59000000\n"
     ]
    }
   ],
   "source": [
    "#Find all numbers in the million_dict sentence\n",
    "for key, text in million_dict.items():\n",
    "    doc = million_dict[key]\n",
    "    match = re.findall(\"([.\\d,]+)\", doc)\n",
    "    million_dict[key] = match\n",
    "\n",
    "#Initalize new dict\n",
    "million_dict_step2 = {}\n",
    "\n",
    "#Loop through and make sure there was only 1 match returned\n",
    "for key, text in million_dict.items():\n",
    "    lis = million_dict[key]\n",
    "    lis_len = len(lis)\n",
    "    if lis_len == 1:\n",
    "        million_dict_step2[key] = lis\n",
    "\n",
    "#Loop through and format, remove all special characters so only thing left in string is numbers\n",
    "for key, text in million_dict_step2.items():\n",
    "    doc = million_dict_step2[key]\n",
    "    doc = str(doc)\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('\"', '')\n",
    "    doc = doc.replace(\"'\", '')\n",
    "    million_dict_step2[key] = doc\n",
    "\n",
    "\n",
    "#Loop through and change numbers into integers, then multiply by a million\n",
    "for key, text in million_dict_step2.items():\n",
    "    item = million_dict_step2[key]\n",
    "    item = float(item)\n",
    "    item = item * 1000000\n",
    "    item = int(item)\n",
    "    \n",
    "    million_dict_step2[key] = item\n",
    "\n",
    "#Print\n",
    "print(million_dict_step2['SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing the same below, as we have above, but for the billions_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000000\n"
     ]
    }
   ],
   "source": [
    "#Find all numbers in the sentence in stored under billion_dict\n",
    "for key, text in billion_dict.items():\n",
    "    doc = billion_dict[key]\n",
    "    match = re.findall(\"([.\\d,]+)\", doc)\n",
    "    billion_dict[key] = match\n",
    "\n",
    "#Create new dictionary\n",
    "billion_dict_step2 = {}\n",
    "\n",
    "#Loop through numbers, if any sentence returned more than one number do not add to billion_dict_step_2, error checking loop\n",
    "for key, text in billion_dict.items():\n",
    "    lis = billion_dict[key]\n",
    "    lis_len = len(lis)\n",
    "    if lis_len == 1:\n",
    "        billion_dict_step2[key] = lis\n",
    "\n",
    "#Loop through numbers and format, remove all special characters so the only thing left in the string is numbers\n",
    "#This loop can be shortened.\n",
    "for key, text in billion_dict_step2.items():\n",
    "    doc = billion_dict_step2[key]\n",
    "    doc = str(doc)\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('\"', '')\n",
    "    doc = doc.replace(\"'\", '')\n",
    "    billion_dict_step2[key] = doc\n",
    "\n",
    "#Loop through the billion_dict_step2 dictionary and convert the strings of numbers into ints, multiply by 1 bill\n",
    "for key, text in billion_dict_step2.items():\n",
    "    item = billion_dict_step2[key]\n",
    "    item = float(item)\n",
    "    item = item * 1000000000\n",
    "    item = int(item)\n",
    "    billion_dict_step2[key] = item\n",
    "\n",
    "    #Print Dict\n",
    "print(billion_dict_step2['Hercules Capital Inc 10-K 2019-02-21.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to combine our results from the million_dict_step2, billion_dict_step2, and likely_dict. We will create a new dict to store these results, all_dict and loop through each dictionary, storing the results in the all_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200000000\n",
      "59000000\n",
      "276496828\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary to store all other dictionaries containing results\n",
    "all_dict = {}\n",
    "\n",
    "#Loop through million_dict_step2 and add all items to all_dict\n",
    "for key, text in million_dict_step2.items():\n",
    "    item = million_dict_step2[key]\n",
    "    all_dict[key] = item\n",
    "    \n",
    "#Loop through billion_dict_step2 and add all items to all_dict\n",
    "for key, text in billion_dict_step2.items():\n",
    "    item = billion_dict_step2[key]\n",
    "    all_dict[key] = item\n",
    "\n",
    "#Loop through likely_dict and add all items to all_dict\n",
    "for key, text in likely_dict.items():\n",
    "    item = likely_dict[key]\n",
    "    all_dict[key] = item\n",
    "\n",
    "#Print all_dict\n",
    "print(all_dict['Hercules Capital Inc 10-K 2019-02-21.txt'])\n",
    "print(all_dict['SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt'])\n",
    "print(all_dict['BSB Bancorp Inc 10-K 2019-03-15.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create a Pandas dataframe for the results. Dataframes can simplify data manipulation and make it easy to save and later load datasets into other Python programs.\n",
    "\n",
    "Once we have created it, we will save it, then print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dollar Value of AMV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>RTI SURGICAL INC 10-K 2019-03-05.txt</td>\n",
       "      <td>286000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CBA Florida Inc 10-K 2019-04-01.txt</td>\n",
       "      <td>6870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt</td>\n",
       "      <td>632000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DITECH HOLDING Corp 10-K 2019-04-16.txt</td>\n",
       "      <td>24100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SecureWorks Corp 10-K 2019-03-28.txt</td>\n",
       "      <td>147300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WEYLAND TECH INC 10-K 2019-04-15.txt</td>\n",
       "      <td>19481155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ABRAXAS PETROLEUM CORP 10-K 2019-03-15.txt</td>\n",
       "      <td>470774656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MARINE PRODUCTS CORP 10-K 2019-02-28.txt</td>\n",
       "      <td>131839491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AMGEN INC 10-K 2019-02-13.txt</td>\n",
       "      <td>119629312769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ROKU INC 10-K 2019-03-01.txt</td>\n",
       "      <td>2857084378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Dollar Value of AMV\n",
       "RTI SURGICAL INC 10-K 2019-03-05.txt                  286000000          \n",
       "CBA Florida Inc 10-K 2019-04-01.txt                   6870000            \n",
       "PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt  632000000          \n",
       "DITECH HOLDING Corp 10-K 2019-04-16.txt               24100000           \n",
       "SecureWorks Corp 10-K 2019-03-28.txt                  147300000          \n",
       "...                                                         ...          \n",
       "WEYLAND TECH INC 10-K 2019-04-15.txt                  19481155           \n",
       "ABRAXAS PETROLEUM CORP 10-K 2019-03-15.txt            470774656          \n",
       "MARINE PRODUCTS CORP 10-K 2019-02-28.txt              131839491          \n",
       "AMGEN INC 10-K 2019-02-13.txt                         119629312769       \n",
       "ROKU INC 10-K 2019-03-01.txt                          2857084378         \n",
       "\n",
       "[63 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dataframe to store results, make 'Dollar Value of AMV' Column Heading\n",
    "Results_df = pd.DataFrame.from_dict(all_dict, orient='index', columns = ['Dollar Value of AMV'])\n",
    "\n",
    "#Save file\n",
    "path = path = join(os.getcwd(), 'Output')\n",
    "Results_df.to_excel(path + 'results.xlsx', index = True)\n",
    "\n",
    "#print dataframe\n",
    "Results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the documents into the Spacy pipeline. The pipeline takes the documents and returns a Spacy object. An object can be called with different commands to return the sentences of the document, the tokens of the document (fancy name for words), and the part of speech of those tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join(os.getcwd(), 'Spacy Files')\n",
    "\n",
    "import spacy\n",
    "#spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Increase memory limit per filing.\n",
    "nlp.max_length = 15000000\n",
    "\n",
    "#Create dict to store spacy files\n",
    "spacy_dict = {}\n",
    "\n",
    "#Process clean text files into spacy documents for better data extraction\n",
    "for filename, document in cleantext_10k_dict.items():\n",
    "    spacy_dict[filename] = nlp(document)\n",
    "    \n",
    "    #Need to create list of documents that already exist, and prevent them from being downloaded again if they do already exist.\n",
    "    html_file = open(join(path, filename), 'w')\n",
    "    resx = spacy_dict[filename]\n",
    "\n",
    "    html_file.write(str(resx))\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a solution to extract AMV information from 10-Ks using Spacy. This code can not be run in Jupyter notebooks and must be run in a Python environment that does not utilize Interactive Python. A safe choice is Spyder. The code below takes documents out of the small_spacy_dict, so you will need to include the code to generate the small_spacy_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will not run in Jupyter, please move to Spyder. \n",
    "# This code is an alteration of:Extracting entity relations @ https://spacy.io/usage/examples\n",
    "import plac\n",
    "import spacy\n",
    "#spacy.require_gpu() #uncomment for GPU\n",
    "\n",
    "\n",
    "\n",
    "#Initalize list to store output\n",
    "output = []\n",
    "\n",
    "#Not sure what this does\n",
    "@plac.annotations(\n",
    "    model=(\"Model to load (needs parser and NER)\", \"positional\", None, str)\n",
    ")\n",
    "\n",
    "#Main function, main function calls the functions below.\n",
    "def main(model=\"en_core_web_lg\"):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "    print(\"Processing %d texts\" % len(TEXTS))\n",
    "\n",
    "    for key, text in small_spacy_dict.items():\n",
    "        doc = nlp(str(text))\n",
    "        relations = extract_currency_relations(doc, key)\n",
    "        output.append(relations)\n",
    "  \n",
    "\n",
    "\n",
    "#This function primarily deals with formatting\n",
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result\n",
    "\n",
    "#This function primarily deals with extracting the relations, it extracts the entities and nouns if the entity type is Money\n",
    "def extract_currency_relations(doc, key):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, money, key))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head, money, key))\n",
    "    return relations\n",
    "\n",
    "#Calls the main function\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "#Output produces a list of tuples for every document that is ran through the function.\n",
    "#Because of this we have a list of list of tuples, we just want a list of tuples, so we are going to make a new list.\n",
    "#We will add all of the tuples to the new list.\n",
    "\n",
    "flat_list = []\n",
    "\n",
    "for sublist in output:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "print(flat_list)\n",
    "\n",
    "#Initalize a list to store our results    \n",
    "result = [] \n",
    "\n",
    "#Now that we have our list of tuples, we will add only the tuples that have the noun value AMV.\n",
    "#we have included two cases of how the noun value of AMV may be present, they are case sensesitive. \n",
    "for i in flat_list:\n",
    "    if str(i[0]) == 'The aggregate market value':\n",
    "        result.append(i)\n",
    "    if str(i[0]) == 'the aggregate market value':\n",
    "        result.append(i)\n",
    "        \n",
    "print(result)   \n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does not run in Jupyter notebooks and must be run in an editor that does not user Interactive Python, such as Spyder. The following code is an alteration of Training NER at https://spacy.io/usage/examples. In this example, the label and training data has been altered for our purposes. There are two challenges when building Spacy models, compute power and datasets. Training a model can take minutes to days depending on the size of the dataset used, the amount of times the training loop is run, and the computer power at your disposal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will not run in Jupyter, please move to Spyder. \n",
    "#This code is an alteration of Training NER @ https://spacy.io/usage/examples\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "# spacy.require_gpu() #uncomment for GPU\n",
    "\n",
    "\n",
    "# new entity label\n",
    "LABEL = \"AMV\"\n",
    "\n",
    "# training data\n",
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"As of June 30, 2018, the last day of the registrant's most recently completed second fiscal quarter, the aggregate market value of the common stock held by non-affiliates of the registrant was $470,774,656 based on the closing sale price as reported on The NASDAQ Stock Market.\", {'entities': [(193, 205, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the registrant's common stock held by non-affiliates of the registrant was $22,262,043,858 as of June 29, 2018 based on the closing sale price of the registrant's common stock on the NASDAQ Global Market on such date.\", {'entities': [(105, 120, \"AMV\")]}),\n",
    "    (\"The aggregate market value of ordinary shares held by non-affiliates on June 30, 2018 was approximately $7.3 billion based on the closing price of such stock on the New York Stock Exchange.\", {'entities': [(104, 116, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the registrant's common stock, $0.01 par value per share Common Stock, held by non-affiliates of the registrant, based on the last sale price of the Common Stock at the close of business on June 29, 2018, was $9,819,826,967.\", {'entities': [(239, 253, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the voting stock held by non-affiliates of the registrant on June 30, 2018, based upon the closing price of $4.07 of the registrant's Class A Common Stock as reported on the NASDAQ Global Select Market, was approximately $3.1 billion, which excludes 87.1 million shares of the registrant's common stock held on June 30, 2018 by then current executive officers, directors, and stockholders that the registrant has concluded are affiliates of the registrant.\", {'entities': [(251, 263, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the shares of Class A Common Stock held by non-affiliates of the registrant, computed by reference to the closing price of such stock as of the last business day of the registrant's most recently completed second quarter, was $7.6 billion.\", {'entities': [(256, 268, \"AMV\")]})\n",
    "]\n",
    "\n",
    "plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "    ner.add_label(\"VEGETABLE\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(100):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = cleantext_10k_dict['BELDEN INC 10-K 2019-02-20.txt']\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
