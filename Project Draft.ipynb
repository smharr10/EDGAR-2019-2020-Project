{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We will begin by importing all of our required modules and downloading the index files from the SEC's website.\n",
    "These index files can be utilized to make a database of filings, information about these filings, and the URLS. Building\n",
    "this database allows us to create samples of filings that we wish to download.\n",
    "\n",
    "After we have imported our modules, we create a dictionary to store URLS and create a variable that contains the base URL.\n",
    "We will utilize a for loop to generate the URLS we will utilize to download the index files. We are utilizing a for loop\n",
    "because the URLS for these filings follow a basic pattern.\n",
    "\n",
    "Once we have generated our dictionary of URLS with the for loop, we create a list to store the files already downloaded in\n",
    "the index folder and a dictionary to store the information about the files we need to download. This will not matter \n",
    "the first time you run the program, but will save you from redownloading the files everytime you run the program in the \n",
    "future.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import spacy\n",
    "import unidecode, requests, unidecode, tqdm\n",
    "import lxml.html\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import os, re, sys, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Url_dict will store filename, URL\n",
    "url_dict = {}\n",
    "url_base = 'https://www.sec.gov/Archives/edgar/full-index/'\n",
    "\n",
    "#Set our data path to the filings folder\n",
    "data_path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#Variables to configure the for loop, loop is configured to only download 2019 files currently.\n",
    "start_year = 2019\n",
    "end_year = 2020\n",
    "date_range = end_year - start_year\n",
    "\n",
    "#loops through each year, then for each year loops through 4 times.\n",
    "for i in range(date_range):\n",
    "    for i in range(1, 5):\n",
    "        dict_key = str(start_year) + '_Q' + str(i)+ '_Master.idx'\n",
    "        url = url_base + str(start_year) + '/QTR' + str(i) +'/master.idx'\n",
    "        url_dict[dict_key] = url\n",
    "        \n",
    "#os.listdir will create a list of files present in the folder of the path it is passed.\n",
    "files_down = os.listdir(data_path)        \n",
    "#Store downloaded files\n",
    "download_dict = {}\n",
    "\n",
    "#Loops through Url_dict and checks if files are in the Index File Foler, if they are not it downloads them and adds them.\n",
    "#.items() is required to loop through dictionaries. We are also required to specificy two variable names before naming\n",
    "#the data structure we wish to alter. For simplicity I usually utilize key, and value. \n",
    "for key, value in url_dict.items():\n",
    "    if key not in files_down:\n",
    "        download_dict[key] = value\n",
    "\n",
    "#Get out data_path\n",
    "data_path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#data_dict will store the raw text data\n",
    "data_dict = {}\n",
    "\n",
    "#Loop through out download dict and download the items contained in it, if we can not get one it will return an error.\n",
    "for key, value in download_dict.items():\n",
    "    #requests.get generates an object. Objects store a variety of information that can be acessed through commands.\n",
    "    res = requests.get(value)\n",
    "    \n",
    "    #Two examples of this are .status code and .txt\n",
    "    if res.status_code == 200:\n",
    "        print('Found...... Downloading  ' + str(key))\n",
    "        res = res.text\n",
    "        data_dict[key] = res\n",
    "        \n",
    "    else:\n",
    "        print('Error.....' + str(key))\n",
    "\n",
    "#Loop through the data_dict and remove everything before CIK, and the ------- in the document. Save to Index File folder\n",
    "#This loop is just for formatting, dont worry about it too much. \n",
    "for key, text in data_dict.items():\n",
    "    text_2 = text.split('CIK')\n",
    "    text = text_2[1]\n",
    "    text = 'CIK' + text\n",
    "    text = re.sub(\"-\",\"\", text, count=80)\n",
    "    filename = key\n",
    "    html_file = open(join(data_path, filename), 'a')\n",
    "    html_file.write(text)\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to take the index files we downloaded and altered, and put them into a dataframe.\n",
    "\n",
    "We will begin by creating another list containing the files downloaded in the Index Files folder. This list is files_0 and we will use this list to gather the names of the files. We will also create a second list files_1, we will use this list to store the actual paths of the files in files_0. We will create these paths by utilizing a for loop, where we can combine the path of the folder with a backslash and the file name to create the path of the file. Interestingly we cannot just add a single backslash to the path and file because a single backslash is a special character in Python, instead we have to pass two backslashes which Python will recognize as one. \n",
    "\n",
    "Once we have the files_1 list we can create a dataframe which will store all of the information from these files. We will do some minor housekeeping with a command to add back leading zeros to the CIK number, reformatting the date from a string to a datetime format incase we want to create a sample based on date. We also need to actually create the URLS for the filings, the index files only come with half of the information needed to create the URL. Luckily we can simply add the first half of the URL which is not unique to the second half of the URL contained within the index filing which is unique. We also do some house keeping with Company names, removing special characters so we can use the company name to create file names later. Pandas will automatically truncate values to save memory, so we will need to stop it from truncating these values.\n",
    "\n",
    "We will finally save the dataframe to the folder in which the program is running from, this will save us from having to repeat this process in the future. We save this a .pkl file which is short for the Pickle file extension. This file extension seralizies data in Python and stores it in a way that it can be loaded into another Python script without having to reformat the information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goopy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "#Set our data path to the filings folder\n",
    "path = join(os.getcwd(), 'Index Files')\n",
    "\n",
    "#Create List of Files in Index Files\n",
    "files_0 = os.listdir(path)\n",
    "\n",
    "#Second List to add path to Index Files\n",
    "files_1 = []\n",
    "\n",
    "#Add Path to Index \n",
    "for file in files_0:\n",
    "    file = path + '\\\\' + file\n",
    "    files_1.append(file)\n",
    "files_1\n",
    "\n",
    "#Create Data Frame\n",
    "Edgar_df = pd.concat((pd.read_table(file, encoding=\"latin1\", sep='|') for file in files_1))\n",
    "\n",
    "#Add back leading zeros\n",
    "Edgar_df['CIK'] = Edgar_df['CIK'].apply(lambda x: '{0:0>10}'.format(x))\n",
    "\n",
    "#Need to Format Date Filed as a datetime, so we can search it later. \n",
    "Edgar_df['Date Filed'] =  pd.to_datetime(Edgar_df['Date Filed'])\n",
    "\n",
    "#Trying adding URL to df, may be able to feed directly into a downloader\n",
    "Edgar_df['URL'] = 'https://www.sec.gov/Archives/' + Edgar_df['Filename']\n",
    "\n",
    "#Remove special characters from the company names, some of these can cause problems\n",
    "Edgar_df['Company Name'].replace('[^A-Za-z0-9- ]+', '', regex=True, inplace=True)\n",
    "\n",
    "#Stops Pandas from truncating values.\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#Save the dataframe so we can open it again later without having to recreate it. \n",
    "Edgar_df.to_pickle('Edgar_df.pkl')\n",
    "\n",
    "print(Edgar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples can be created using this dataframe but for the purpose this tutorial we are going to use a preselected sample. This is because later on we will be trying to extract Aggregate Market Value from 10-K filings. This data has structure to it, but the structure differs over time and betweeen companies, therefore we will be controlling for varaibles in this tutorial. In theory the script can be extended to other 10-Ks without issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load predefined sample\n",
    "Sample_df = pd.read_pickle('Sample_df.pkl')\n",
    "\n",
    "# Update Index for Identifer, with an Identifer the df will play nicely with flow control\n",
    "Sample_df = Sample_df.reset_index(drop=True)\n",
    "Sample_df.index.name = 'File ID'\n",
    "\n",
    "#Convert Date Filed back to a string so we can combine it with Company Name, and Form Type\n",
    "#to get our save name.\n",
    "Sample_df['Date Filed'] = Sample_df['Date Filed'].astype(str)\n",
    "Sample_df['Save_Name'] = Sample_df['Company Name'] + ' ' + Sample_df['Form Type'] + ' ' + Sample_df['Date Filed'] + '.txt'\n",
    "print(Sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to download the 10-K filings from the SEC website.\n",
    "\n",
    "We start be defining a function, download_file. This function will take the url, date, form, and company name, and return the text and status code. This function was orginally from Dr. Kok's github, and has been modifed for our purposes.\n",
    "\n",
    "After we have defined our download function we will get ready to download the files. We will begin by setting our path equal to the path of the folder where we will store our filings. This will allow us to check for any filings present in the folder, and tell our program where to store the files once we have downloaded them.\n",
    "\n",
    "We also need to create two dictionaries to store the information for use in our program. I personally utilize dictionaries alot in my programs because information can be stored by an identifier called a key. You may for example store a cleaned 10-k in a dictionary using the company name as the key. This gives you a unique identifier to call the information as opposed to a list where you can only call stored information by the index where it is stored. Dictionaries and lists can be combined however to store multipule pieces of inforamtion under one key. I have found however it is often better to use multipule dictionaries than to combine datastructures as combining datastructures creates some unique problems, and messy code. Sometimes it is necessary and more convient, but it is rare.\n",
    "\n",
    "Once we have intalized our datastructures and have our path, and files list, we can start downloading our files. We will create a for loop for this task, once again we will need to pass two variables before we declare the datastructure we wish to alter. We will also have to do something new with this for loop, we will need to call the function .iterrows() for our datastructure, this is because we are dealing with a dataframe object and not a dictionary, the .itterows() function, functions simillarly to the .items() function. The for loop contains two if statements, we want to check if the file is saved in our index folder, and if it is we will open the file, and if it is not we will download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to downlaod files\n",
    "def download_file(url, date, form, company, max_tries=4, sleep_time = 1):\n",
    "    failed_attempts = 0\n",
    "    while True: \n",
    "        res = requests.get(url)\n",
    "        \n",
    "        #download the raw html of the file we just scraped\n",
    "        if '.txt' in url:\n",
    "            # Define filename\n",
    "            filename = company + ' ' + form + ' ' + date + '.txt'\n",
    "            \n",
    "            #Create file\n",
    "            html_file = open(join(data_path, filename), 'a')\n",
    "            \n",
    "            #Decode res object\n",
    "            resx = unidecode.unidecode(res.text)\n",
    "            \n",
    "            #Save file\n",
    "            html_file.write(str(resx))\n",
    "            \n",
    "            #Close file\n",
    "            html_file.close()\n",
    "            \n",
    "        #Status_code = 200 means sucessful scraping\n",
    "        if res.status_code == 200:  \n",
    "            return True, res.text\n",
    "        \n",
    "        #Loop will attempt to download 3 more times, if failure has been encountered \n",
    "        else:\n",
    "            if failed_attempts < max_tries:\n",
    "                failed_attempts += 1\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                return False, 'Could not download'\n",
    "\n",
    "\n",
    "#Set our data path to the filings folder\n",
    "data_path = join(os.getcwd(), 'Filings')\n",
    "\n",
    "#List of filenames in the filing folder\n",
    "files_down = os.listdir(data_path)\n",
    "\n",
    "result_10k_dict = {}\n",
    "status_dict = {}\n",
    "\n",
    "for index, row in Sample_df.iterrows():\n",
    "        \n",
    "    #Load file list incase program is used out of order    \n",
    "    files_down = os.listdir(data_path)\n",
    "    \n",
    "    #if program exists, load it instead of downloading it again\n",
    "    if row['Save_Name'] in files_down:\n",
    "        with open(join(data_path, row['Save_Name']), 'r') as file:\n",
    "            file_content = file.read()\n",
    "            result_10k_dict[row['Save_Name']] = file_content\n",
    "            \n",
    "    #if program does not exist then call the download function    \n",
    "    if row['Save_Name'] not in files_down:\n",
    "        download_res = download_file(row['URL'], row['Date Filed'], row['Form Type'], row['Company Name'])\n",
    "    \n",
    "        status_dict[index] = download_res[0]\n",
    "    \n",
    "        if download_res[0]:\n",
    "            result_10k_dict[row['Save_Name']] = download_res[1]\n",
    "                   \n",
    "#The if not in x part of the loop works correctly and the files download, need to test after they have been saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code is largely taken from Dr. Kok's github. It is the pipeline to process the files we have downloaded into clean text. I haven't spent a large amount of time on this section of code, and therefore my understanding of it is currently subpar.\n",
    "\n",
    "We start be defining a pattern_dict, this dictionary contains regular expressions statements. \n",
    "\n",
    "Regular expressions are a very valuable, and very frustrating tool. They are confusing and unintuitve, and they will only make sense after practice, like organic chemistry. I would recommend reading the Python documentation for regular expressions here: https://docs.python.org/3/library/re.html.\n",
    "\n",
    "We will use this dictionary to extract metadata from 10-k documents. We do this through the extract_metadata function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goopy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bda45660764da8b4e6e6af8f1531fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goopy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968f70d3a514418baad079cd1ffe3962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "#Define regular expressions dictionary\n",
    "pattern_dict = {\n",
    "    'documents' : re.compile(r\"<document>(.*?)</document>\", re.IGNORECASE | re.DOTALL),\n",
    "    'metadata' : {\n",
    "        'type' : re.compile(r\"<type>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'sequence' : re.compile(r\"<sequence>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'Filename' : re.compile(r\"<filename>(.*?)\\n\", re.IGNORECASE | re.DOTALL),\n",
    "        'description' : re.compile(r\"<description>(.*?)\\n\", re.IGNORECASE | re.DOTALL)\n",
    "    },\n",
    "    'text' : re.compile(r\"<text>(.*?)</text>\", re.IGNORECASE | re.DOTALL)\n",
    "}\n",
    "\n",
    "#Define extract_metadata function\n",
    "def extract_metadata(doc, pattern_dict=pattern_dict):\n",
    "    data_dict = {}\n",
    "    \n",
    "    data_dict['metadata'] = {}\n",
    "    for key, pattern in pattern_dict['metadata'].items():\n",
    "        matches = pattern.findall(doc)\n",
    "        if matches:\n",
    "            data_dict['metadata'][key] = matches[0]\n",
    "        else:\n",
    "            data_dict['metadata'][key] = np.nan\n",
    "            \n",
    "    text_match = pattern_dict['text'].findall(doc)\n",
    "    if text_match:\n",
    "        data_dict['text'] = text_match[0]\n",
    "    else:\n",
    "        data_dict['text'] = np.nan\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "data_10k_dict = {}\n",
    "for Filename, data in result_10k_dict.items():\n",
    "    docs_split = pattern_dict['documents'].findall(data)\n",
    "        \n",
    "    for doc in docs_split:\n",
    "        doc_data = extract_metadata(doc)\n",
    "        \n",
    "        ## Only keep 10-K document\n",
    "        if doc_data['metadata']['type'] == '10-K':\n",
    "            data_10k_dict[Filename] = doc_data['text']\n",
    "            break\n",
    "            \n",
    "html_10k_dict = {}\n",
    "text_10k_dict = {}\n",
    "for Filename, raw_text in tqdm(data_10k_dict.items()):\n",
    "    html = lxml.html.fromstring(raw_text)\n",
    "    html_10k_dict[Filename] = html\n",
    "    text_10k_dict[Filename] = html.text_content()\n",
    "    \n",
    "path = join(os.getcwd(), 'Clean Files')\n",
    "cleantext_10k_dict = {}\n",
    "for Filename, text in tqdm(text_10k_dict.items()):\n",
    "    ## Fix encoding\n",
    "    clean_text = unidecode.unidecode(text)\n",
    "    \n",
    "    ## Replace newline characters with space\n",
    "    clean_text = re.sub('\\s', ' ', clean_text)\n",
    "    \n",
    "    ## Remove duplicate whitespaces\n",
    "    clean_text = ' '.join([word for word in clean_text.split(' ') if word])\n",
    "    \n",
    "    ## Replace \"Page number + Table of Contents footer\"\n",
    "    clean_text = re.sub(' \\d+ Table of Contents ', ' ', clean_text)\n",
    "    \n",
    "    cleantext_10k_dict[Filename] = clean_text\n",
    "    \n",
    "    html_file = open(join(path, Filename), 'w')\n",
    "    resx = unidecode.unidecode(text)\n",
    "\n",
    "    html_file.write(str(resx))\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to shorten the cleantext files we just produced and store them in new dictionary, appropriately named cleantext_10k_dict_short. This is because the length of time it will take to run our program is primarily based on the amount of text our program will have to process. Since the AMV is listed early in the 10-K filing we can safely get rid of 4/5s of the 10-K filing without risking data loss. This will speed up our program immensly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext_10k_dict_short = {}\n",
    "\n",
    "#Loop through cleantext_10k_dict and take the first 1/5 of the document and store it in cleantext_10k_dict_short\n",
    "for key, text in cleantext_10k_dict.items():\n",
    "    doc = str(cleantext_10k_dict[key])\n",
    "    length = len(doc)\n",
    "    new_len = length * .2\n",
    "    new_len = int(float(new_len))\n",
    "    doc = doc[0:new_len]\n",
    "    cleantext_10k_dict_short[key] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the cleantext and search for the aggregate market value in the text using a for loop and a regular expression. We start the loop by creating a variable to hold our text, doc, and set doc equal to the cleantext passed through a str() function. This is because the cleantext is currently being stored as an requests object. Once we have the doc as a string we can also pass the .lower() function, which changes any capital letters to lower case letters, this way we do not need to worry about case senesitivity. We then define our regular expressions in the result variable, we give the regular expressions two cases to search for seperated by the '|' character. The first expression tells the program to search for aggregate market value, a number, a decimal, two more numbers, then any 7 characters. This is meant to look for numbers such as 7.0 billion or 8.00 million. The second regular expression just looks for a sentence containing 'aggregate market value' and returns that sentence. We then create an if statement to eliminate any text documents that did not return a result. If a result was not found the function will return '[]' therefore we can tell the if statement if the result does not equal '[]' then we want to store that result in our amv dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amv_dict = {}\n",
    "#Loops through the short dict and searchs for the aggregate market value in sentences\n",
    "for key, item in cleantext_10k_dict_short.items():\n",
    "    #Get String of the doc\n",
    "    doc = str(cleantext_10k_dict_short[key])\n",
    "    \n",
    "    #Make the doc lowercase so we don't have to worry about case sensitivity\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    #Search for aggergate market value followed by X.X XXXXXXXX then just aggregate market value in a sentence\n",
    "    #First case will return the million/billion values if present\n",
    "    result = re.findall(r\"([^.]*?aggregate market value[^.]*\\.[0-9]........|[^.]*?aggregate market value[^.]*\\.)\", doc)\n",
    "    \n",
    "    #Only save if we have a result\n",
    "    if str(result) != '[]':\n",
    "        amv_dict[key] = result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now stored the sentence that likely contains the aggregate market value information in the amv_dictionary. We now need to start extracting the numbers from that sentence. This is quite easy for the sentences that say $7.0 billion or $8 million, but quite difficult for the ones that say: Our AMV was approximately $1,302,302 based on a total share number of 304,303, trading at X price. We will cover the millions and billions right now and pick up this conversation in the next code segment.\n",
    "\n",
    "We are going to create 4 dictionaries to store information. The functions and data collection in the following section can and will be cleaned up significantly. We are going to use match_dict to store all of the numbers we extract from these sentences, we are going to use match_dict_mb to store strings such as '$4 million' and $8 billion', million_dict will be used to store the strings that are in denominations of millions, and billions_dict will be used to store the string that are in the denominations of billions.\n",
    "\n",
    "We will use a for loop and regular expression to extract the the strings in the form of '$4 million' and '$8 billion' from the amv_dictionary. We will then store these in the match_dict_mb and use another for loop and a couple of if statements to sort these into the million_dict and the billion_dict. We will sort these simply by checking if 'mill' or 'bill' is in the text itself. We use a shorter string than 'million' or 'billion' because these filings often contain typos.\n",
    "\n",
    "We finish by using a for loop to extract any number present in a sentence from the amv_dict and store these numbers in the match_dict. We will use this match_dict to attempt to extract the AMV from the rest of the companies. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {}\n",
    "match_dict_mb = {}\n",
    "million_dict = {}\n",
    "billion_dict = {}\n",
    "\n",
    "#Search the amv_dict for the millions and billions value and store them in match_dict_mb\n",
    "for key, text in amv_dict.items():\n",
    "    doc = str(amv_dict[key])\n",
    "    match = re.findall(r\"(\\$[.\\d,]+........)\", doc)\n",
    "    if match:\n",
    "        match_dict_mb[key] = str(match)\n",
    "\n",
    "#Loop through match_dict_mb if mill in the sentence store in million dict, if bill store in billions dict\n",
    "for key, text in match_dict_mb.items():\n",
    "    doc = str(match_dict_mb[key])\n",
    "    if 'mill' in text:\n",
    "        million_dict[key] = text\n",
    "    if 'bill' in text:\n",
    "        billion_dict[key] = text\n",
    "        \n",
    "#Extract just the numerical values from all the sentences in amv_dict\n",
    "for key, text in amv_dict.items():\n",
    "    doc = str(amv_dict[key])\n",
    "    match = re.findall(r\"([.\\d,]+)\", doc)\n",
    "    if match:\n",
    "        match_dict[key] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through match_dict and remove special characters\n",
    "#Can clean and shorten this\n",
    "for key, text in match_dict.items():\n",
    "    item_lis = []\n",
    "    for item in text:\n",
    "        doc = item\n",
    "        doc = str(doc)\n",
    "        doc = doc.replace('[', '')\n",
    "        doc = doc.replace('[', '')\n",
    "        doc = doc.replace(']', '')\n",
    "        doc = doc.replace('\"', '')\n",
    "        doc = doc.replace(\"'\", '')\n",
    "        doc = doc.replace(\" \", '|')\n",
    "        doc = doc.replace(\",\", '')\n",
    "        doc = doc.replace(r\".\", '')\n",
    "        doc = str(doc)\n",
    "        doc = str(doc)\n",
    "        item_lis.append(doc)\n",
    "        match_dict[key] = item_lis\n",
    "\n",
    "#Loop through match_dict delete blank values, and convert integers to spaces\n",
    "for key,text in match_dict.items():\n",
    "    doc = match_dict[key]\n",
    "    num_lis = []\n",
    "    for item in doc:\n",
    "        if item == ' ' or item == '':\n",
    "            item = int(0)\n",
    "            num_lis.append(item)\n",
    "        else:\n",
    "            item = int(item)\n",
    "            num_lis.append(item)\n",
    "    num_lis = list(map(int, num_lis))\n",
    "    num_lis.sort(reverse=True)\n",
    "    num = num_lis[0]\n",
    "    match_dict[key] = num\n",
    "\n",
    "likely_dict = {}\n",
    "    \n",
    "for key, text in match_dict.items():\n",
    "    item = match_dict[key]\n",
    "    if item > 10000:\n",
    "        likely_dict[key] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RTI SURGICAL INC 10-K 2019-03-05.txt': ['286.0'], 'CBA Florida Inc 10-K 2019-04-01.txt': ['6.87'], 'PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt': ['632'], 'DITECH HOLDING Corp 10-K 2019-04-16.txt': ['24.1'], 'SecureWorks Corp 10-K 2019-03-28.txt': ['147.3'], 'I-Minerals Inc 10-K 2019-07-29.txt': ['6.0'], 'STONERIDGE INC 10-K 2019-02-28.txt': ['961.3'], 'Metropolitan Bank Holding Corp 10-K 2019-03-13.txt': ['340.2'], 'LA JOLLA PHARMACEUTICAL CO 10-K 2019-03-04.txt': ['603.7'], 'SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt': ['59.0'], 'TechTarget Inc 10-K 2019-03-13.txt': ['516.1']}\n",
      "{'RTI SURGICAL INC 10-K 2019-03-05.txt': ['286.0'], 'CBA Florida Inc 10-K 2019-04-01.txt': ['6.87'], 'PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt': ['632'], 'DITECH HOLDING Corp 10-K 2019-04-16.txt': ['24.1'], 'SecureWorks Corp 10-K 2019-03-28.txt': ['147.3'], 'I-Minerals Inc 10-K 2019-07-29.txt': ['6.0'], 'STONERIDGE INC 10-K 2019-02-28.txt': ['961.3'], 'Metropolitan Bank Holding Corp 10-K 2019-03-13.txt': ['340.2'], 'LA JOLLA PHARMACEUTICAL CO 10-K 2019-03-04.txt': ['603.7'], 'SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt': ['59.0'], 'TechTarget Inc 10-K 2019-03-13.txt': ['516.1']}\n",
      "{'RTI SURGICAL INC 10-K 2019-03-05.txt': '286.0', 'CBA Florida Inc 10-K 2019-04-01.txt': '6.87', 'PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt': '632', 'DITECH HOLDING Corp 10-K 2019-04-16.txt': '24.1', 'SecureWorks Corp 10-K 2019-03-28.txt': '147.3', 'I-Minerals Inc 10-K 2019-07-29.txt': '6.0', 'STONERIDGE INC 10-K 2019-02-28.txt': '961.3', 'Metropolitan Bank Holding Corp 10-K 2019-03-13.txt': '340.2', 'LA JOLLA PHARMACEUTICAL CO 10-K 2019-03-04.txt': '603.7', 'SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt': '59.0', 'TechTarget Inc 10-K 2019-03-13.txt': '516.1'}\n",
      "{'RTI SURGICAL INC 10-K 2019-03-05.txt': 286000000, 'CBA Florida Inc 10-K 2019-04-01.txt': 6870000, 'PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt': 632000000, 'DITECH HOLDING Corp 10-K 2019-04-16.txt': 24100000, 'SecureWorks Corp 10-K 2019-03-28.txt': 147300000, 'I-Minerals Inc 10-K 2019-07-29.txt': 6000000, 'STONERIDGE INC 10-K 2019-02-28.txt': 961300000, 'Metropolitan Bank Holding Corp 10-K 2019-03-13.txt': 340200000, 'LA JOLLA PHARMACEUTICAL CO 10-K 2019-03-04.txt': 603700000, 'SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt': 59000000, 'TechTarget Inc 10-K 2019-03-13.txt': 516100000}\n"
     ]
    }
   ],
   "source": [
    "#Find all numbers in the million_dict sentence\n",
    "for key, text in million_dict.items():\n",
    "    doc = million_dict[key]\n",
    "    match = re.findall(\"([.\\d,]+)\", doc)\n",
    "    million_dict[key] = match\n",
    "print(million_dict)\n",
    "\n",
    "#Initalize new dict\n",
    "million_dict_step2 = {}\n",
    "\n",
    "#Loop through and make sure there was only 1 match returned\n",
    "for key, text in million_dict.items():\n",
    "    lis = million_dict[key]\n",
    "    lis_len = len(lis)\n",
    "    if lis_len == 1:\n",
    "        million_dict_step2[key] = lis\n",
    "print(million_dict_step2)\n",
    "\n",
    "#Loop through and format, remove all special characters so only thing left in string is numbers\n",
    "for key, text in million_dict_step2.items():\n",
    "    doc = million_dict_step2[key]\n",
    "    doc = str(doc)\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('\"', '')\n",
    "    doc = doc.replace(\"'\", '')\n",
    "    million_dict_step2[key] = doc\n",
    "print(million_dict_step2)\n",
    "\n",
    "#Loop through and change numbers into integers, then multiply by a million\n",
    "for key, text in million_dict_step2.items():\n",
    "    item = million_dict_step2[key]\n",
    "    item = float(item)\n",
    "    item = item * 1000000\n",
    "    item = int(item)\n",
    "    \n",
    "    million_dict_step2[key] = item\n",
    "\n",
    "#Print\n",
    "print(million_dict_step2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hercules Capital Inc 10-K 2019-02-21.txt': ['1.2'], 'CROWN CASTLE INTERNATIONAL CORP 10-K 2019-02-25.txt': ['44.6'], 'TC PIPELINES LP 10-K 2019-02-21.txt': ['1.4'], 'MOODYS CORP DE 10-K 2019-02-25.txt': ['32.7'], 'Cheniere Energy Partners LP 10-K 2019-02-26.txt': ['8.8'], 'ZEBRA TECHNOLOGIES CORP 10-K 2019-02-14.txt': ['7.6'], 'National Vision Holdings Inc 10-K 2019-02-27.txt': ['1.3'], 'METTLER TOLEDO INTERNATIONAL INC 10-K 2019-02-08.txt': ['14.6'], 'IBERIABANK CORP 10-K 2019-02-22.txt': ['4.2'], 'SEATTLE GENETICS INC WA 10-K 2019-02-07.txt': ['7.0'], 'CHURCH  DWIGHT CO INC DE 10-K 2019-02-21.txt': ['12.5'], 'Allegion plc 10-K 2019-02-19.txt': ['7.3'], 'Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt': ['1.5']}\n",
      "{'Hercules Capital Inc 10-K 2019-02-21.txt': ['1.2'], 'CROWN CASTLE INTERNATIONAL CORP 10-K 2019-02-25.txt': ['44.6'], 'TC PIPELINES LP 10-K 2019-02-21.txt': ['1.4'], 'MOODYS CORP DE 10-K 2019-02-25.txt': ['32.7'], 'Cheniere Energy Partners LP 10-K 2019-02-26.txt': ['8.8'], 'ZEBRA TECHNOLOGIES CORP 10-K 2019-02-14.txt': ['7.6'], 'National Vision Holdings Inc 10-K 2019-02-27.txt': ['1.3'], 'METTLER TOLEDO INTERNATIONAL INC 10-K 2019-02-08.txt': ['14.6'], 'IBERIABANK CORP 10-K 2019-02-22.txt': ['4.2'], 'SEATTLE GENETICS INC WA 10-K 2019-02-07.txt': ['7.0'], 'CHURCH  DWIGHT CO INC DE 10-K 2019-02-21.txt': ['12.5'], 'Allegion plc 10-K 2019-02-19.txt': ['7.3'], 'Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt': ['1.5']}\n",
      "{'Hercules Capital Inc 10-K 2019-02-21.txt': '1.2', 'CROWN CASTLE INTERNATIONAL CORP 10-K 2019-02-25.txt': '44.6', 'TC PIPELINES LP 10-K 2019-02-21.txt': '1.4', 'MOODYS CORP DE 10-K 2019-02-25.txt': '32.7', 'Cheniere Energy Partners LP 10-K 2019-02-26.txt': '8.8', 'ZEBRA TECHNOLOGIES CORP 10-K 2019-02-14.txt': '7.6', 'National Vision Holdings Inc 10-K 2019-02-27.txt': '1.3', 'METTLER TOLEDO INTERNATIONAL INC 10-K 2019-02-08.txt': '14.6', 'IBERIABANK CORP 10-K 2019-02-22.txt': '4.2', 'SEATTLE GENETICS INC WA 10-K 2019-02-07.txt': '7.0', 'CHURCH  DWIGHT CO INC DE 10-K 2019-02-21.txt': '12.5', 'Allegion plc 10-K 2019-02-19.txt': '7.3', 'Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt': '1.5'}\n",
      "{'Hercules Capital Inc 10-K 2019-02-21.txt': 1200000000, 'CROWN CASTLE INTERNATIONAL CORP 10-K 2019-02-25.txt': 44600000000, 'TC PIPELINES LP 10-K 2019-02-21.txt': 1400000000, 'MOODYS CORP DE 10-K 2019-02-25.txt': 32700000000, 'Cheniere Energy Partners LP 10-K 2019-02-26.txt': 8800000000, 'ZEBRA TECHNOLOGIES CORP 10-K 2019-02-14.txt': 7600000000, 'National Vision Holdings Inc 10-K 2019-02-27.txt': 1300000000, 'METTLER TOLEDO INTERNATIONAL INC 10-K 2019-02-08.txt': 14600000000, 'IBERIABANK CORP 10-K 2019-02-22.txt': 4200000000, 'SEATTLE GENETICS INC WA 10-K 2019-02-07.txt': 7000000000, 'CHURCH  DWIGHT CO INC DE 10-K 2019-02-21.txt': 12500000000, 'Allegion plc 10-K 2019-02-19.txt': 7300000000, 'Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt': 1500000000}\n"
     ]
    }
   ],
   "source": [
    "#Find all numbers in the sentence in stored under billion_dict\n",
    "for key, text in billion_dict.items():\n",
    "    doc = billion_dict[key]\n",
    "    match = re.findall(\"([.\\d,]+)\", doc)\n",
    "    billion_dict[key] = match\n",
    "print(billion_dict)\n",
    "\n",
    "#Create new dictionary\n",
    "billion_dict_step2 = {}\n",
    "\n",
    "#Loop through numbers, if any sentence returned more than one number do not add to billion_dict_step_2, error checking loop\n",
    "for key, text in billion_dict.items():\n",
    "    lis = billion_dict[key]\n",
    "    lis_len = len(lis)\n",
    "    if lis_len == 1:\n",
    "        billion_dict_step2[key] = lis\n",
    "print(billion_dict_step2)\n",
    "\n",
    "#Loop through numbers and format, remove all special characters so the only thing left in the string is numbers\n",
    "#This loop can be shortened.\n",
    "for key, text in billion_dict_step2.items():\n",
    "    doc = billion_dict_step2[key]\n",
    "    doc = str(doc)\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('\"', '')\n",
    "    doc = doc.replace(\"'\", '')\n",
    "    billion_dict_step2[key] = doc\n",
    "print(billion_dict_step2)\n",
    "\n",
    "#Loop through the billion_dict_step2 dictionary and convert the strings of numbers into ints, multiply by 1 bill\n",
    "for key, text in billion_dict_step2.items():\n",
    "    item = billion_dict_step2[key]\n",
    "    item = float(item)\n",
    "    item = item * 1000000000\n",
    "    item = int(item)\n",
    "    \n",
    "    billion_dict_step2[key] = item\n",
    "\n",
    "    #Print Dict\n",
    "print(billion_dict_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RTI SURGICAL INC 10-K 2019-03-05.txt': 286000000, 'CBA Florida Inc 10-K 2019-04-01.txt': 6870000, 'PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt': 632000000, 'DITECH HOLDING Corp 10-K 2019-04-16.txt': 24100000, 'SecureWorks Corp 10-K 2019-03-28.txt': 147300000, 'I-Minerals Inc 10-K 2019-07-29.txt': 6000000, 'STONERIDGE INC 10-K 2019-02-28.txt': 961300000, 'Metropolitan Bank Holding Corp 10-K 2019-03-13.txt': 340200000, 'LA JOLLA PHARMACEUTICAL CO 10-K 2019-03-04.txt': 603700000, 'SECOND SIGHT MEDICAL PRODUCTS INC 10-K 2019-03-19.txt': 59000000, 'TechTarget Inc 10-K 2019-03-13.txt': 516100000, 'Hercules Capital Inc 10-K 2019-02-21.txt': 1200000000, 'CROWN CASTLE INTERNATIONAL CORP 10-K 2019-02-25.txt': 44600000000, 'TC PIPELINES LP 10-K 2019-02-21.txt': 1400000000, 'MOODYS CORP DE 10-K 2019-02-25.txt': 32700000000, 'Cheniere Energy Partners LP 10-K 2019-02-26.txt': 8800000000, 'ZEBRA TECHNOLOGIES CORP 10-K 2019-02-14.txt': 7600000000, 'National Vision Holdings Inc 10-K 2019-02-27.txt': 1300000000, 'METTLER TOLEDO INTERNATIONAL INC 10-K 2019-02-08.txt': 14600000000, 'IBERIABANK CORP 10-K 2019-02-22.txt': 4200000000, 'SEATTLE GENETICS INC WA 10-K 2019-02-07.txt': 7000000000, 'CHURCH  DWIGHT CO INC DE 10-K 2019-02-21.txt': 12500000000, 'Allegion plc 10-K 2019-02-19.txt': 7300000000, 'Fortress Transportation  Infrastructure Investors LLC 10-K 2019-02-28.txt': 1500000000, 'BSB Bancorp Inc 10-K 2019-03-15.txt': 276496828, 'USANA HEALTH SCIENCES INC 10-K 2019-02-26.txt': 1581107286, 'HMG COURTLAND PROPERTIES INC 10-K 2019-03-28.txt': 6071006, 'DECKERS OUTDOOR CORP 10-K 2019-05-30.txt': 3468302000, 'MORGAN STANLEY 10-K 2019-02-26.txt': 79320949858, 'Whitestone REIT 10-K 2019-03-15.txt': 490696053, 'Longwen Group Corp 10-K 2019-04-15.txt': 60394, 'WILLAMETTE VALLEY VINEYARDS INC 10-K 2019-03-21.txt': 37932314, 'At Home Group Inc 10-K 2019-03-27.txt': 1202740687, 'Starco Brands Inc 10-K 2019-04-01.txt': 46193294, 'PACIFIC BLUE ENERGY CORP 10-K 2019-04-30.txt': 127206, 'PORTSMOUTH SQUARE INC 10-K 2019-08-30.txt': 5919000, 'OLIN CORP 10-K 2019-02-25.txt': 4774200502, 'Stellus Capital Investment Corp 10-K 2019-03-06.txt': 192472866, 'AEI INCOME  GROWTH FUND 24 LLC 10-K 2019-03-29.txt': 23531792, 'Teladoc Health Inc 10-K 2019-02-27.txt': 3986708558, 'REPUBLIC FIRST BANCORP INC 10-K 2019-03-14.txt': 411031864, 'CORE MOLDING TECHNOLOGIES INC 10-K 2019-03-18.txt': 65146916, 'ALIGN TECHNOLOGY INC 10-K 2019-02-28.txt': 22262043858, 'PTC INC 10-K 2019-11-18.txt': 10784576792, 'AMMO INC 10-K 2019-07-01.txt': 70697432, 'HACKETT GROUP INC 10-K 2019-03-08.txt': 340823370, 'SEVERN BANCORP INC 10-K 2019-04-18.txt': 8824205, 'W O Group Inc 10-K 2019-04-15.txt': 125396341, 'NATURES SUNSHINE PRODUCTS INC 10-K 2019-03-08.txt': 80636000, 'FRANKLIN WIRELESS CORP 10-K 2019-09-30.txt': 10410000, 'NovaBay Pharmaceuticals Inc 10-K 2019-03-29.txt': 12878993, 'Propanc Biopharma Inc 10-K 2019-10-15.txt': 11285202, 'IntelGenx Technologies Corp 10-K 2019-03-22.txt': 46205996, 'TRAVELZOO 10-K 2019-03-11.txt': 98522556, 'Four Corners Property Trust Inc 10-K 2019-02-20.txt': 1543211886, 'PACIFIC BLUE ENERGY CORP 10-K 2019-05-01.txt': 127206, 'VOYA RETIREMENT INSURANCE  ANNUITY Co 10-K 2019-03-14.txt': 55000, 'IONIS PHARMACEUTICALS INC 10-K 2019-03-01.txt': 4805287142, 'WEYLAND TECH INC 10-K 2019-04-15.txt': 19481155, 'ABRAXAS PETROLEUM CORP 10-K 2019-03-15.txt': 470774656, 'MARINE PRODUCTS CORP 10-K 2019-02-28.txt': 131839491, 'AMGEN INC 10-K 2019-02-13.txt': 119629312769, 'ROKU INC 10-K 2019-03-01.txt': 2857084378}\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary to store all other dictionaries containing results\n",
    "all_dict = {}\n",
    "\n",
    "#Loop through million_dict_step2 and add all items to all_dict\n",
    "for key, text in million_dict_step2.items():\n",
    "    item = million_dict_step2[key]\n",
    "    all_dict[key] = item\n",
    "    \n",
    "#Loop through billion_dict_step2 and add all items to all_dict\n",
    "for key, text in billion_dict_step2.items():\n",
    "    item = billion_dict_step2[key]\n",
    "    all_dict[key] = item\n",
    "\n",
    "#Loop through likely_dict and add all items to all_dict\n",
    "for key, text in likely_dict.items():\n",
    "    item = likely_dict[key]\n",
    "    all_dict[key] = item\n",
    "\n",
    "#Print all_dict\n",
    "print(all_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dollar Value of AMV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RTI SURGICAL INC 10-K 2019-03-05.txt</th>\n",
       "      <td>286000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBA Florida Inc 10-K 2019-04-01.txt</th>\n",
       "      <td>6870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt</th>\n",
       "      <td>632000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DITECH HOLDING Corp 10-K 2019-04-16.txt</th>\n",
       "      <td>24100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SecureWorks Corp 10-K 2019-03-28.txt</th>\n",
       "      <td>147300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEYLAND TECH INC 10-K 2019-04-15.txt</th>\n",
       "      <td>19481155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABRAXAS PETROLEUM CORP 10-K 2019-03-15.txt</th>\n",
       "      <td>470774656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARINE PRODUCTS CORP 10-K 2019-02-28.txt</th>\n",
       "      <td>131839491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMGEN INC 10-K 2019-02-13.txt</th>\n",
       "      <td>119629312769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROKU INC 10-K 2019-03-01.txt</th>\n",
       "      <td>2857084378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Dollar Value of AMV\n",
       "RTI SURGICAL INC 10-K 2019-03-05.txt                  286000000          \n",
       "CBA Florida Inc 10-K 2019-04-01.txt                   6870000            \n",
       "PEAPACK GLADSTONE FINANCIAL CORP 10-K 2019-03-14.txt  632000000          \n",
       "DITECH HOLDING Corp 10-K 2019-04-16.txt               24100000           \n",
       "SecureWorks Corp 10-K 2019-03-28.txt                  147300000          \n",
       "...                                                         ...          \n",
       "WEYLAND TECH INC 10-K 2019-04-15.txt                  19481155           \n",
       "ABRAXAS PETROLEUM CORP 10-K 2019-03-15.txt            470774656          \n",
       "MARINE PRODUCTS CORP 10-K 2019-02-28.txt              131839491          \n",
       "AMGEN INC 10-K 2019-02-13.txt                         119629312769       \n",
       "ROKU INC 10-K 2019-03-01.txt                          2857084378         \n",
       "\n",
       "[63 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dataframe to store results, make 'Dollar Value of AMV' Column Heading\n",
    "Results_df = pd.DataFrame.from_dict(all_dict, orient='index', columns = ['Dollar Value of AMV'])\n",
    "\n",
    "#print dataframe\n",
    "print(Results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path = join(os.getcwd(), 'Output')\n",
    "Results_df.to_excel(path + 'results.xlsx', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the documents into the Spacy pipline. The pipeline takes the documents and returns a Spacy object. An object can be called with different commands to return the sentences of the document, the tokens of the document (fancy name for words), and the part of speech of those tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goopy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a29e0e8884a82a184fdb4af3eb67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = join(os.getcwd(), 'Spacy Files')\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Increase memory limit per filing.\n",
    "nlp.max_length = 15000000\n",
    "\n",
    "#Create dict to store spacy files\n",
    "spacy_dict = {}\n",
    "\n",
    "#Process clean text files into spacy documents for better data extraction\n",
    "for filename, document in tqdm(cleantext_10k_dict.items()):\n",
    "    spacy_dict[filename] = nlp(document)\n",
    "    \n",
    "    #Need to create list of documents that already exist, and prevent them from being downloaded again if they do already exist.\n",
    "    html_file = open(join(path, filename), 'w')\n",
    "    resx = spacy_dict[filename]\n",
    "\n",
    "    html_file.write(str(resx))\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a solution to extract AMV information from 10-Ks using Spacy. This code can not be run in Jupyter notebooks and must be ran in a Python environment that does not utilize Interactive Python. A safe choice is Spyder. The code below takes documents out of the small_spacy_dict, so you will need to include the code to generate the small_spacy_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [model]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goopy\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#This code will not run in Jupyter, please move to Spyder. \n",
    "# This code is an alteration of:Extracting entity relations @ https://spacy.io/usage/examples\n",
    "import plac\n",
    "import spacy\n",
    "spacy.require_gpu() #uncomment for GPU\n",
    "\n",
    "\n",
    "\n",
    "#Initalize list to store output\n",
    "output = []\n",
    "\n",
    "#Not sure what this does\n",
    "@plac.annotations(\n",
    "    model=(\"Model to load (needs parser and NER)\", \"positional\", None, str)\n",
    ")\n",
    "\n",
    "#Main function, main function calls the functions below.\n",
    "def main(model=\"en_core_web_lg\"):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "    print(\"Processing %d texts\" % len(TEXTS))\n",
    "\n",
    "    for key, text in small_spacy_dict.items():\n",
    "        doc = nlp(str(text))\n",
    "        relations = extract_currency_relations(doc, key)\n",
    "        output.append(relations)\n",
    "  \n",
    "\n",
    "\n",
    "#This function primarily deals with formatting\n",
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result\n",
    "\n",
    "#This function primarily deals with extracting the relations, it extracts the entities and nouns if the entity type is Money\n",
    "def extract_currency_relations(doc, key):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, money, key))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head, money, key))\n",
    "    return relations\n",
    "\n",
    "#Calls the main function\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "#Output produces a list of tuples for every document that is ran through the function.\n",
    "#Because of this we have a list of list of tuples, we just want a list of tuples, so we are going to make a new list.\n",
    "#We will add all of the tuples to the new list.\n",
    "\n",
    "flat_list = []\n",
    "\n",
    "for sublist in output:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "print(flat_list)\n",
    "\n",
    "#Initalize a list to store our results    \n",
    "result = [] \n",
    "\n",
    "#Now that we have our list of tuples, we will add only the tuples that have the noun value AMV.\n",
    "#we have included two cases of how the noun value of AMV may be present, they are case sensesitive. \n",
    "for i in flat_list:\n",
    "    if str(i[0]) == 'The aggregate market value':\n",
    "        result.append(i)\n",
    "    if str(i[0]) == 'the aggregate market value':\n",
    "        result.append(i)\n",
    "        \n",
    "print(result)   \n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does not run in Jupyter notebooks and must be run in an editor that does not user Interactive Python, such as Spyder. The following code is an alteration of Training NER at https://spacy.io/usage/examples. In this example the label and training data has been altered for our purposes. There are two challenges when building Spacy models, compute power, and datasets. Training a model can take minutes to days depending on the size of the dataset used, the amount of times the training loop is ran, and the comptuer power at your disposal. In my experiements it has been taking me about an hour to train a model on a 2080ti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will not run in Jupyter, please move to Spyder. \n",
    "#This code is an alteration of Training NER @ https://spacy.io/usage/examples\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "# spacy.require_gpu() #uncomment for GPU\n",
    "\n",
    "\n",
    "# new entity label\n",
    "LABEL = \"AMV\"\n",
    "\n",
    "# training data\n",
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\"As of June 30, 2018, the last day of the registrant's most recently completed second fiscal quarter, the aggregate market value of the common stock held by non-affiliates of the registrant was $470,774,656 based on the closing sale price as reported on The NASDAQ Stock Market.\", {'entities': [(193, 205, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the registrant's common stock held by non-affiliates of the registrant was $22,262,043,858 as of June 29, 2018 based on the closing sale price of the registrant's common stock on the NASDAQ Global Market on such date.\", {'entities': [(105, 120, \"AMV\")]}),\n",
    "    (\"The aggregate market value of ordinary shares held by non-affiliates on June 30, 2018 was approximately $7.3 billion based on the closing price of such stock on the New York Stock Exchange.\", {'entities': [(104, 116, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the registrant's common stock, $0.01 par value per share Common Stock, held by non-affiliates of the registrant, based on the last sale price of the Common Stock at the close of business on June 29, 2018, was $9,819,826,967.\", {'entities': [(239, 253, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the voting stock held by non-affiliates of the registrant on June 30, 2018, based upon the closing price of $4.07 of the registrant's Class A Common Stock as reported on the NASDAQ Global Select Market, was approximately $3.1 billion, which excludes 87.1 million shares of the registrant's common stock held on June 30, 2018 by then current executive officers, directors, and stockholders that the registrant has concluded are affiliates of the registrant.\", {'entities': [(251, 263, \"AMV\")]}),\n",
    "    (\"The aggregate market value of the shares of Class A Common Stock held by non-affiliates of the registrant, computed by reference to the closing price of such stock as of the last business day of the registrant's most recently completed second quarter, was $7.6 billion.\", {'entities': [(256, 268, \"AMV\")]})\n",
    "]\n",
    "\n",
    "plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "    ner.add_label(\"VEGETABLE\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(100):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = cleantext_10k_dict['BELDEN INC 10-K 2019-02-20.txt']\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
